{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create and Push a Batch Run to AWS\n",
    "\n",
    "**WARNING**: Make sure everything is set correctly BEFORE running this notebook!  Since this notebook starts processing on the AWS servers, it has the potential  to consume a lot of resources, aka dollars...\n",
    "\n",
    "This notebook:\n",
    "* Imports the ASF database query file we created in step 1\n",
    "* chooses a set of interferograms to process\n",
    "* prepares the files needed to process those interferograms\n",
    "* uploads files to AWS servers and starts the processing\n",
    "\n",
    "Requires the following files to be in the folder where you run the code:\n",
    "* apmb.geojson (file with coordinates for APMB)\n",
    "* query.geojson (file with results of ASF database query)\n",
    "* template.yml (yaml file with ISCE processing parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Import Python packages\n",
    "\n",
    "Import the packages needed to run this notebook.  You may need to install the dinosar package first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dinosar library not in base environment uncomment below (run just once)\n",
    "#!pip install --no-cache git+https://github.com/scottyhq/dinosar.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dinosar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-14aced82519d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdinosar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dinosar'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import dinosar\n",
    "import geopandas as gpd\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Choose processing parameters\n",
    "\n",
    "Make sure you have this section set the way you want it before running later cells!  \n",
    "\n",
    "In this section we'll set:\n",
    "* Which interferograms to make\n",
    "* ISCE processing parameters (swaths, filters, etc.)\n",
    "* AWS processing parameters (S3 bucket name, AWS job name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Choose which interferograms to process\n",
    "This section sets the track to process, and makes pairs from one date each month for all the data from that track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which track to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose track to process\n",
    "track = 83\n",
    "# load the ASF search results that you generated in Step 1\n",
    "gf = dinosar.archive.asf.load_inventory('query.geojson')\n",
    "# create a new dataframe with only the track selected, in the date bounds selected\n",
    "gdf=gf.query('relativeOrbit == @track')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is currently set up to then make sequential pairs with one date from each month.  However, that can be changed in future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create yet another dataframe with only some info so we can quickly check that we selected the data we actually want\n",
    "df = gdf.loc[:,['frameNumber','dateStamp','relativeOrbit']].sort_values(by='dateStamp')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frameNumber</th>\n",
       "      <th>dateStamp</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dateStamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-03</th>\n",
       "      <td>1105</td>\n",
       "      <td>2014-11-03</td>\n",
       "      <td>Sentinel-1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-27</th>\n",
       "      <td>1110</td>\n",
       "      <td>2014-11-27</td>\n",
       "      <td>Sentinel-1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-21</th>\n",
       "      <td>1105</td>\n",
       "      <td>2014-12-21</td>\n",
       "      <td>Sentinel-1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14</th>\n",
       "      <td>1104</td>\n",
       "      <td>2015-01-14</td>\n",
       "      <td>Sentinel-1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-07</th>\n",
       "      <td>1105</td>\n",
       "      <td>2015-02-07</td>\n",
       "      <td>Sentinel-1A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           frameNumber  dateStamp     platform\n",
       "dateStamp                                     \n",
       "2014-11-03        1105 2014-11-03  Sentinel-1A\n",
       "2014-11-27        1110 2014-11-27  Sentinel-1A\n",
       "2014-12-21        1105 2014-12-21  Sentinel-1A\n",
       "2015-01-14        1104 2015-01-14  Sentinel-1A\n",
       "2015-02-07        1105 2015-02-07  Sentinel-1A"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only unique dates and set DateTimeIndex\n",
    "df  = df.drop_duplicates('dateStamp').set_index('dateStamp', drop=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will create pairs with one acquisition from each month\n",
    "def one_per_month(df):\n",
    "    # Single date per month\n",
    "    tmp = df.resample('M').first()\n",
    "    tmp1 = tmp.sort_index(ascending=False)\n",
    "    dates = tmp1.dateStamp.apply(lambda x: x.strftime('%Y%m%d')).to_list()\n",
    "    pairs = dates_to_pairs(dates)\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pairs\n",
    "pairs = one_per_month(df)\n",
    "# print the number of pairs and a list of the pairs\n",
    "print(f'{len(pairs)} Interferograms to generate:')\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2: Set ISCE Processing Parameters\n",
    "\n",
    "1. Open the \"template.yml\" file and save a copy with a new name for this batch run (e.g., \"Template_83_JanFeb2019.yml\").\n",
    "2. In the new .yml file, change the swath numbers to match the track you want to process:\n",
    "  * 156: 1,2\n",
    "  * 149: 2,3\n",
    "  * 76: 1,2\n",
    "  * 83: 2,3\n",
    "3. Optional - change other ISCE processing parameters in the template as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the name of the .yml template file here\n",
    "template=\"Template_83_JanFeb2019.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3: Set AWS parameters\n",
    "These are the parameters you need to set for each job you push to AWS:\n",
    "* Destination for processing files on AWS (folder name)\n",
    "* Job name - to track your job while it is processing\n",
    "\n",
    "Ideally, these names should tell you something about the processing job (i.e., \"Track83_2014-2020_1month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws parameters:\n",
    "dirname = 'D83-JanFeb2019' #name of the folder on AWS where your processing files will be\n",
    "jobname = 'uturuncu-D83-TEST' #name for the job on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Create processing directories and push them to AWS\n",
    "\n",
    "Now that we've defined our processing parameters and chosen the interferograms to process, we'll create a processing directory for each interferogram, and then upload those directories to the AWS server (S3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set full path to bucket on S3\n",
    "bucket = 's3://dinosar/processing/uturuncu/D83-JanFeb2019' + dirname\n",
    "# create a text file with the interferogram pairs to process\n",
    "pairsFile = 'pairs.txt'\n",
    "\n",
    "paths = [bucket+'/'+x for x in pairs]\n",
    "with open(pairsFile, 'w') as f:\n",
    "    f.write('\\n'.join(paths))\n",
    "\n",
    "cmd = f'aws s3 cp {pairsFile} {bucket}/{pairsFile}'#write the command to push pairs.txt to the AWS bucket\n",
    "\n",
    "print(cmd)\n",
    "\n",
    "subprocess.call(cmd, shell=True) # runs the command to push the files with pairs to process to the AWS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pairsFile) as f:\n",
    "    pairs = [line.rstrip() for line in f]\n",
    "    mapping = dict(enumerate(pairs))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = 'prep_topsApp_local'\n",
    "for i,p in enumerate(pairs):\n",
    "    intname = os.path.basename(p)\n",
    "    junk,master,slave=intname.split('-')\n",
    "    intdir = f'int-{master}-{slave}'\n",
    "    cmd = f'{script} -i query.geojson -m {master} -p {relOrbit} -s {slave} -t {template}'\n",
    "    print(i, cmd)\n",
    "    subprocess.call(cmd, shell=True) # runs the command to make processing directories on the local system  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a processing directory for each interferogram in this directory.  Each processing directory should have two files:\n",
    "* topsApp.xml  = input file for ISCE processing\n",
    "* download-links.txt = text file with the links to download all the data we'll need for processing\n",
    "\n",
    "Now we push all those directories to the S3 cloud storage on AWS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move these to cloud storage\n",
    "# Push folder of text files to S3\n",
    "for i,p in enumerate(pairs):\n",
    "    intname = os.path.basename(p)\n",
    "    junk,master,slave=intname.split('-')\n",
    "    intdir = f'int-{master}-{slave}'\n",
    "    cmd = f'aws s3 sync {intdir}/ {bucket}/{intdir}/'\n",
    "    print(cmd)\n",
    "    subprocess.call(cmd, shell=True)\n",
    "\n",
    "print(f'Moved files to {bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: Launch Processing on AWS (WARNING: can consume lots of AWS resources!!)\n",
    "Now that we have all the files we need for processing on the AWS servers, we can start processing!  Don't run these cells until you're *SURE* you've got the interferograms you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your NASA URS password to download SLCs\n",
    "nasauser = 'pmacqueen' # NASA EarthData username\n",
    "nasapass = getpass.getpass() # NASA EarthData password (will create an interactive textbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change these:\n",
    "demDir = 's3://dinosar/processing/uturuncu/dem' #where the DEM is stored on AWS\n",
    "jobdef = 'uturuncu-array' # sets certain parameters on AWS\n",
    "jobqueue = 'uturuncu-queue' # sets certain parameters on AWS\n",
    "array_size = len(pairs)\n",
    "\n",
    "\n",
    "# NOTE: job-name, job-queue, and job-definition are JSON files that I've created for AWS Batch\n",
    "# The specify type of computers to use, etc\n",
    "cmd = f\"aws batch submit-job \\\n",
    "--job-name {jobname} \\\n",
    "--job-queue {jobqueue} \\\n",
    "--job-definition {jobdef} \\\n",
    "--array-properties size={array_size} \\\n",
    "--parameters 'S3_PAIRS={bucket}/{pairsFile},S3_DEM={demDir}' \\\n",
    "--container-overrides 'environment=[{{name=NASAUSER,value={nasauser}}},{{name=NASAPASS,value={nasapass}}}]' \\\n",
    "\"\n",
    "\n",
    "# WARNING: this prints your password as plain text, careful not to push to github\n",
    "# If you run the command in terminal sometimes the error messages are more helpful\n",
    "#print(cmd) # uncomment to print the command for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start processing!\n",
    "aws_batchout = subprocess.check_output(cmd, shell=True) # runs the AWS command to start the batch job\n",
    "aws_batchout # prints out the job-id - make a note of the job id for finding the logs later!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a text file with a summary of this job for reference later\n",
    "sumfilename = 'summary_' + jobname + '.txt'\n",
    "\n",
    "with open(sumfilename, 'w') as sf:\n",
    "    sf.write('track = '+ str(track) + '\\n')\n",
    "    sf.write('template file = '+ template + '\\n')\n",
    "    sf.write('S3 Bucket Name = ' + dirname + '\\n')\n",
    "    sf.write('Job Name = ' + jobname)\n",
    "    sf.write('Job Name and Job ID on AWS = ' + aws_batchout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5: Wait for the jobs to finish!\n",
    "\n",
    "* The end products will be in \"s3://dinosar/results/uturuncu/(dirname)/(int_dir)/merged\"\n",
    "* Check the file \"summary_(jobname).txt\" for a summary of the processing parameters you set in this script\n",
    "* You can monitor jobs here: https://us-west-2.console.aws.amazon.com/batch/home?region=us-west-2#/jobs/queue/arn:aws:batch:us-west-2:783380859522:job-queue~2Futuruncu-queue?state=PENDING\n",
    "* After 24 hours, you'll have to look up your job here using the job id: https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/batch/job;streamFilter=typeLogStreamPrefix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
