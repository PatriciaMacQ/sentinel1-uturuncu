{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create and Push a Batch Run to AWS\n",
    "\n",
    "**WARNING**: Make sure everything is set correctly BEFORE running this notebook!  Since this notebook starts processing on the AWS servers, it has the potential  to consume a lot of resources, aka dollars...\n",
    "\n",
    "This notebook:\n",
    "* Imports the ASF database query file we created in step 1\n",
    "* chooses a set of interferograms to process\n",
    "* prepares the files needed to process those interferograms\n",
    "* uploads files to AWS servers and starts the processing\n",
    "\n",
    "Requires the following files to be in the folder where you run the code:\n",
    "* apmb.geojson (file with coordinates for APMB)\n",
    "* query.geojson (file with results of ASF database query)\n",
    "* template.yml (yaml file with ISCE processing parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Import Python packages\n",
    "\n",
    "Import the packages needed to run this notebook.  You may need to install the dinosar package first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dinosar library not in base environment uncomment below (run just once)\n",
    "#!pip install --no-cache git+https://github.com/scottyhq/dinosar.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import dinosar\n",
    "import geopandas as gpd\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Choose processing parameters\n",
    "\n",
    "Make sure you have this section set the way you want it before runnng later cells!  \n",
    "\n",
    "In this section we'll set:\n",
    "* Track\n",
    "* Time span to cover (start and end times)\n",
    "* AWS processing parameters (S3 bucket name, AWS job name)\n",
    "* ISCE processing parameters (swaths, filters, etc.)\n",
    "* How to choose the interferograms to make (explained more later!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2: Set data subset and AWS parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters:\n",
    "# i.e., which subset of all the stuff on AWS do you want to process\n",
    "track = 83\n",
    "start_date = '01/01/2019'\n",
    "end_date = '03/01/2019'\n",
    "\n",
    "# aws parameters:\n",
    "dirname = 'D83-JanFeb2019' #name of the folder on AWS where your processing files will be\n",
    "jobname = 'uturuncu-D83-JanFeb2019' #name for the job on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://dinosar/processing/uturuncu/D83-JanFeb2019D83-JanFeb2019'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 's3://dinosar/processing/uturuncu/D83-JanFeb2019' + foldername\n",
    "bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3: Set ISCE Processing Parameters\n",
    "\n",
    "Open the \"template.yml\" file and save a copy with a new name for this batch run (e.g., \"Template_83_JanFeb2019.yml\").  Change the processing parameters if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the name of the template file here\n",
    "template=\"Template_83_JanFeb2019.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Choose which interferograms to process\n",
    "Still working on this section.  Trying to decide how I want to do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ASF search results\n",
    "gf = dinosar.archive.asf.load_inventory('query.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frameNumber</th>\n",
       "      <th>dateStamp</th>\n",
       "      <th>relativeOrbit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>670</td>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>660</td>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>665</td>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>660</td>\n",
       "      <td>2019-01-24</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>665</td>\n",
       "      <td>2019-01-24</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     frameNumber  dateStamp  relativeOrbit\n",
       "1705         670 2019-01-12             83\n",
       "1707         660 2019-01-12             83\n",
       "1706         665 2019-01-12             83\n",
       "1696         660 2019-01-24             83\n",
       "1695         665 2019-01-24             83"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new dataframe with only the track selected, in the date bounds selected\n",
    "gdf=gf.query('dateStamp > @start_date & dateStamp < @end_date').query('relativeOrbit == @track')\n",
    "# create yet another datafram with only some info so we can quickly check that we selected the data we actually want\n",
    "df = gdf.loc[:,['frameNumber','dateStamp','relativeOrbit']].sort_values(by='dateStamp')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Create processing directories and push them to AWS\n",
    "\n",
    "Now that we've defined our processing parameters and indentified the interferograms to process, we'll create a processing directory for each interferogram, and then upload those directories to the AWS server (S3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9cdf9a623be5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpairsFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pairs.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairsFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# set full path to bucket on S3\n",
    "bucket = 's3://dinosar/processing/uturuncu/D83-JanFeb2019' + dirname\n",
    "# create a text file with the interferogram pairs to process\n",
    "pairsFile = 'pairs.txt'\n",
    "\n",
    "paths = [bucket+'/'+x for x in pairs]\n",
    "with open(pairsFile, 'w') as f:\n",
    "    f.write('\\n'.join(paths))\n",
    "\n",
    "cmd = f'aws s3 cp {pairsFile} {bucket}/{pairsFile}'#write the command to push pairs.txt to the AWS bucket\n",
    "\n",
    "print(cmd)\n",
    "\n",
    "#subprocess.call(cmd, shell=True)  # Uncomment to actually run command    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pairsFile) as f:\n",
    "    pairs = [line.rstrip() for line in f]\n",
    "    mapping = dict(enumerate(pairs))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = 'prep_topsApp_local'\n",
    "for i,p in enumerate(pairs):\n",
    "    intname = os.path.basename(p)\n",
    "    junk,master,slave=intname.split('-')\n",
    "    intdir = f'int-{master}-{slave}'\n",
    "    cmd = f'{script} -i query.geojson -m {master} -p {relOrbit} -s {slave} -t {template}'\n",
    "    print(i, cmd)\n",
    "    #subprocess.call(cmd, shell=True) # Uncomment to actually run command  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a processing directory for each interferogram in this directory.  Each processing directory should have two files:\n",
    "* topsApp.xml  = input file for ISCE processing\n",
    "* download-links.txt = text file with the links to download all the data we'll need for processing\n",
    "\n",
    "Now we push all those directories to the S3 cloud storage on AWS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move these to cloud storage\n",
    "# Push folder of text files to S3\n",
    "for i,p in enumerate(pairs):\n",
    "    intname = os.path.basename(p)\n",
    "    junk,master,slave=intname.split('-')\n",
    "    intdir = f'int-{master}-{slave}'\n",
    "    cmd = f'aws s3 sync {intdir}/ {bucket}/{intdir}/'\n",
    "    print(cmd)\n",
    "    subprocess.call(cmd, shell=True)\n",
    "\n",
    "print(f'Moved files to {bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: Launch Processing on AWS (WARNING: can consume lots of AWS resources!!)\n",
    "Now that we have all the files we need for processing on the AWS servers, we can start processing!  Don't run these cells until you're *SURE* you've got the interferograms you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your NASA URS password to download SLCs\n",
    "nasauser = 'pmacqueen' # NASA EarthData username\n",
    "nasapass = getpass.getpass() # NASA EarthData password (will create an interactive textbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change these:\n",
    "demDir = 's3://dinosar/processing/uturuncu/dem' #where the DEM is stored on AWS\n",
    "jobdef = 'uturuncu-array' # sets certain parameters on AWS\n",
    "jobqueue = 'uturuncu-queue' # sets certain parameters on AWS\n",
    "array_size = len(pairs)\n",
    "\n",
    "\n",
    "# NOTE: job-name, job-queue, and job-definition are JSON files that I've created for AWS Batch\n",
    "# The specify type of computers to use, etc\n",
    "cmd = f\"aws batch submit-job \\\n",
    "--job-name {jobname} \\\n",
    "--job-queue {jobqueue} \\\n",
    "--job-definition {jobdef} \\\n",
    "--array-properties size={array_size} \\\n",
    "--parameters 'S3_PAIRS={bucket}/{pairsFile},S3_DEM={demDir}' \\\n",
    "--container-overrides 'environment=[{{name=NASAUSER,value={nasauser}}},{{name=NASAPASS,value={nasapass}}}]' \\\n",
    "\"\n",
    "\n",
    "# warning: this prints your password as plain text, careful not to push to github\n",
    "# If you run the command in terminal sometimes the error messages are more helpful\n",
    "#print(cmd) # uncomment to print the command for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below and run this cell to start processing!\n",
    "#subprocess.check_output(cmd, shell=True) \n",
    "# prints out the job-id - make a note of the job id for finding the logs later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5: Wait for the jobs to finish!\n",
    "\n",
    "* The end products will be in \"s3://dinosar/results/uturuncu/(dirname)/(int_dir)/merged\"\n",
    "* You can monitor jobs here: https://us-west-2.console.aws.amazon.com/batch/home?region=us-west-2#/jobs/queue/arn:aws:batch:us-west-2:783380859522:job-queue~2Futuruncu-queue?state=PENDING\n",
    "* After 24 hours, you'll have to look up your job here using the job id: https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/batch/job;streamFilter=typeLogStreamPrefix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
